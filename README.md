# <img height=34 src="https://raw.githubusercontent.com/Tarikul-Islam-Anik/Animated-Fluent-Emojis/master/Emojis/Hand%20gestures/Waving%20Hand.png"/> Awesome-Vison2Audio
A curated list of Vison to Audio Generation


<img height=220 src="./img/v2m.png"/>

##  <img height=34 src="https://raw.githubusercontent.com/Tarikul-Islam-Anik/Animated-Fluent-Emojis/master/Emojis/Smilies/Smiling%20Face%20with%20Sunglasses.png"/> Paper List


### 2025
- 2025 ğŸ”‰ VAFlow: Video-to-Audio Generation with Cross-Modality Flow Matching, ICCV'25
- 2025 ğŸ”‰ Foley-Flow: Coordinated Video-to-Audio Generation with Masked Audio-Visual Alignment and Dynamic Conditional Flows, CVPR'25
- 2025 ğŸ¶ GVMGen: A General Video-to-Music Generation Model with Hierarchical Attentions
- 2025 ğŸ¶ AudioX: Diffusion Transformer for Anything-to-Audio Generation
- 2025 ğŸ¶ FilmComposer: LLM-Driven Music Production for Silent Film Clips, CVPR'25
- 2025 ğŸ¶ HarmonySet: A Comprehensive Dataset for Understanding Video-Music Semantic Alignment and Temporal Synchronization, CVPR'25
- 2025 ğŸ¶ Extending Visual Dynamics for Video-to-Music Generation
- 2025 ğŸ”‰ Synchronized Video-to-Audio Generation via Mel Quantization-Continuum Decomposition, CVPR'25

### 2024
- 2024  ğŸ¶ VMAs: Video-to-Music Generation via Semantic Alignment in Web Music Videos, ByteDance, [ğŸŒ Demo](https://genjib.github.io/project_page/VMAs/index.html)
- 2024  ğŸ¶ MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation, Institute of automation, MM'24
- 2024 Jul. ğŸ”‰ FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds. Shanghai Artificial Intelligence Laboratory, Chinese University of Hong Kong, Shenzhen. [ğŸŒ Demo](https://foleycrafter.github.io/) [ğŸ”— Code](https://github.com/open-mmlab/FoleyCrafter) [ğŸ¤— HF](https://huggingface.co/ymzhang319/FoleyCrafter) Datasets (VGGSound, AVSync15)
- 2024 Jul. ğŸ”‰ FRIEREN: Efficient Video-to-Audio Generation with Rectified Flow Matching. ZheJiang University. [ğŸŒ Demo](https://frieren-v2a.github.io/) Datastes (VGGSound)
- 2024 Jul. ğŸ”‰ Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity. Dolby Lab. ECCV'24. [ğŸŒ Demo](https://maskvat.github.io/) Datastes (VGGSound)
- 2024 Jul. ğŸ”‰ Read, Watch and Scream! Sound Generation from Text and Video, NAVER. [ğŸŒ Demo](https://naver-ai.github.io/rewas/) Datastes (VGGSound)
- 2024 June. ğŸ¶ VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling, HKUST, Microsoft Research Asia.  [ğŸ”— Code](https://github.com/ZeyueT/VidMuse/) Datasets (V2M).
- 2024 May. ğŸ”‰ Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation, Sony. [image2audio]. [ğŸŒ Demo](https://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/edit#slide=id.g2cca3e60f2e_1_118) Datasets (VGGSound)
- 2024 Feb. ğŸ”‰ Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion  Latent Aligners, HKUST, CVPR'24. [ğŸŒ Demo](https://yzxing87.github.io/Seeing-and-Hearing/) [ğŸ”— Code](https://github.com/yzxing87/Seeing-and-Hearing) Datasets (VGGSound)
- 2024 ğŸ¶ V2Meow: Meowing to the Visual Beat via Video-to-Music Generation, Google. AAAI'24. [ğŸŒ Demo](https://google-research.github.io/noise2music/v2meow/) Dataset (MV100K)
- 2024 ğŸ¶ Diff-BGM: A Diffusion Model for Video Background Music Generation, PKU, CVPR'24. [ğŸ”— Code](https://github.com/sizhelee/Diff-BGM) Datasets (BGM909)
- 2024 ğŸ¶ MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models, Adobe, CVPR'24. [ğŸ”— Code](https://github.com/schowdhury671/melfusion/tree/main) [ğŸŒ Demo](https://schowdhury671.github.io/melfusion_cvpr2024/)
- 2024 ğŸ”‰ From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation, University of Washington, ICML'24. 
- 2024 ğŸ”‰ SonicVisionLM: Playing Sound with Vision Language Models, Shanghai University, CVPR'24 [ğŸŒ Demo](https://yusiissy.github.io/SonicVisionLM.github.io/)
- 2024 ğŸ¶ Video2Music: Suitable music generation from videos using an Affective Multimodal Transformer model, SUTD, EXPERT SYST APPL'249. [ğŸ”— Code](https://github.com/AMAAI-Lab/Video2Music). Datasets (MuVi-Sync)
- 2024 ğŸ”‰ V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models, Dolby, AAAI'24. [ğŸŒ Demo](https://v2a-mapper.github.io/)
- 2024 ğŸ¶ DanceComposer: Dance-to-Music Generation Using a Progressive Conditional Music Generator, Sun Yat-sen University, TMM'24
- 2024 ğŸ”‰ Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event Condition For Foley Sound, KAIST. [ğŸŒ Demo](https://jnwnlee.github.io/video-foley-demo/)
- 2024 ğŸ”‰ LoVA: Long-form Video-to-Audio Generation, RUC. Datasets(AudioSet, VGGSound, UnAV100)
- 2024 ğŸ”‰ Tell What You Hear From What You See - Video to Audio Generation Through Text, University of Washington, NeurIPS'24 [ğŸ”— Code](https://github.com/DragonLiu1995/multimodal-llm-for-audio-gen)

### 2023
- 2023 Aug. ğŸ¶ Video Background Music Generation: Dataset, Method and Evaluation, Beihang University, ICCV'23. [ğŸ”— Code](https://github.com/zhuole1025/SymMV) Datasets (SymMV)
- 2023 Jun. ğŸ”‰ DiffFoley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models, Tsinghua University, NIPS'23. [ğŸŒ Demo](https://diff-foley.github.io/) [ğŸ”— Code](https://github.com/luosiallen/Diff-Foley) Datasets (VGGSound, AudioSet)
- 2023 Feb. ğŸ¶ Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation, Illinois Institute of Technology, ICLR'23. [ğŸŒ Demo](https://l-yezhu.github.io/CDCD/) [ğŸ”— Code](https://github.com/L-YeZhu/CDCD) Datasets (AIST++, Tiktok Dance-Music)
- 2023 ğŸ”‰ MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation, Renmin University of China CVPR'23. [ğŸ”— Code](https://github.com/researchmm/MM-Diffusion) Datasets (Landscape, AIST++)
- 2023 ğŸ”‰ Conditional Generation of Audio from Video via Foley Analogies, University of Michigan, Adobe, CVPR'23 [ğŸŒ Demo](https://xypb.github.io/CondFoleyGen/) [ğŸ”— Code](https://github.com/XYPB/CondFoleyGen)
- 2023 ğŸ¶ Long-Term Rhythmic Video Soundtracker,Shanghai Artificial Intelligence Laboratory, ICML'23 [ğŸŒ Demo](https://justinyuu.github.io/LORIS/) [ğŸ”— Code](https://github.com/OpenGVLab/LORIS)
- 2023 ğŸ”‰ CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling, Microsoft

### 2022 
- 2022 Jul. ğŸ¶ Quantized GAN for Complex Music Generation from Dance Videos, Illinois Institute of Technology, ECCV'22. [ğŸŒ Demo](https://l-yezhu.github.io/D2M-GAN/) [ğŸ”— Code](https://github.com/L-YeZhu/D2M-GAN) Datasets (AIST++, Tiktok Dance-Music)

### 2021
- 2021 Nov. ğŸ¶ Video Background Music Generation with Controllable Music Transformer, Beihang University, MM'21. [ğŸŒ Demo](https://wzk1015.github.io/cmt/) [ğŸ”— Code](https://github.com/wzk1015/video-bgm-generation) ğŸŒŸ
- 2021 ğŸ”‰ How Does it Sound? Generation of Rhythmic Soundtracks for Human Movement Videos. University of Washington, NeurIPS'21 [ğŸ”— Code](https://github.com/shlizee/RhythmicNet)
  
### 2020
- 2020 Jul. ğŸ”‰ Generating Visually Aligned Sound from Videos, South China University of Technology, TIP'20. [ğŸŒ Demo](https://www.youtube.com/watch?v=fI_h5mZG7bg) [ğŸ”— Code](https://github.com/PeihaoChen/regnet) 
- 2020 Jul. ğŸ¶ Foley Music: Learning to Generate Music from Videos, MIT.
- 2020 Jun. ğŸ”‰ Audeo: Audio Generation for a Silent Performance Video, University of Washington.

### 2019
- 2019, ğŸ¶ AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing, AIST, ISMIR'19. 

### 2018
- 2018 Jun. ğŸ”‰ Visual to Sound: Generating Natural Sound for Videos in the Wild, University of North Carolina, CVPR'18. 
- 2018, ğŸ”‰ Visually Indicated Sound Generation by Perceptually Optimized Classification, University of Southern California, ECCV MULA workshop'18. [ğŸ”— Code](https://github.com/kanchen-usc/VIG)

### 2016
- 2016 ğŸ”‰ Visually Indicated Sounds, MIT, CVPR'16. [ğŸŒ Demo](https://andrewowens.com/vis/) [ğŸ”— Code](https://github.com/GeorgeEfstathiadis/Visually-Indicated-Sounds)

## <img height=34 src="https://raw.githubusercontent.com/Tarikul-Islam-Anik/Animated-Fluent-Emojis/master/Emojis/Smilies/Face%20with%20Monocle.png"/> Survey
- 2025 Mar. Vision-to-Music Generation: A Survey.  [ğŸ”— Code](https://github.com/wzk1015/Awesome-Vision-to-Music-Generation).
- 2024 Aug. Foundation Models for Music: A Survey, Queen Mary University of London.
- 2024 Jun. LLMs Meet Multimodal Generation and Editing:  A Survey, HKUST. [ğŸ”— Code](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation).
- 2022 Aug. Learning in Audio-visual Context: A Review, Analysis, and New Perspective, Renmin University of China.
- 2023 Sep. Sparks of Large Audio Models: A Survey and Outlook, Queensland University of Technology.

## <img height=34 src="https://raw.githubusercontent.com/Tarikul-Islam-Anik/Animated-Fluent-Emojis/master/Emojis/Activities/Teddy%20Bear.png"/> Datasets

- ğŸ¶ V2M (Unpublished): VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling. (Movie trailer, 190K for training, 20K for finetuning, 300 for benchmarking).
- ğŸ”‰ [VGGSound](https://huggingface.co/datasets/Speech-Audio/VGGSOUND/tree/main): Vggsound: A large-scale audio-visual dataset. ICASSP'20
- Landscape
- ğŸ¶ [Tiktok Dance Dataset](https://www.kaggle.com/datasets/yasaminjafarian/tiktokdataset)
- ğŸ”‰ AVSync15: Audio-synchronized visual animation.
- ğŸ¶ [BGM909](https://sizhelee.github.io/publication/bgm909.html). Piano version music. Diff-BGM: A Diffusion Model for Video Background Music Generation
- ğŸ¶ MV100K: V2Meow
- ğŸ¶ [MMTrailer](https://huggingface.co/datasets/litwell/MMTrail-20M): A Multimodal Trailer Video Dataset with Language and Music Descriptions.
- ğŸ¶ [SymMV](https://github.com/zhuole1025/SymMV/tree/main/dataset): Video Background Music Generation: Dataset, Method and Evaluation.
- ğŸ”‰ [VAS](https://drive.google.com/file/d/14birixmH7vwIWKxCHI0MIWCcZyohF59g/view): Generating Visually Aligned Sound from Videos.
- ğŸ¶ [AIST++](https://google.github.io/aistplusplus_dataset/download.html). Dance-to-Music
- ğŸ¶ [AIST](https://aistdancedb.ongaaccel.jp/). Dance-to-Music
- ğŸ¶ [MuVi-Sync](https://zenodo.org/records/10057093). Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model


## <img height=34 src="https://raw.githubusercontent.com/Tarikul-Islam-Anik/Animated-Fluent-Emojis/master/Emojis/Travel%20and%20places/Roller%20Coaster.png"/> Evaluation Metrics

- [Frechet Audio Distance, KL](https://github.com/haoheliu/audioldm_eval)
- [Density, Coverage](https://github.com/clovaai/generative-evaluation-prdc)
- Video-Music CLIP (Precision)/CLIPScore
- [Music Quality (mgeval)](https://github.com/RichardYang40148/mgeval): Rhythms, Genre, Coherence, Quality ....
- [ImagebindScore](https://github.com/facebookresearch/ImageBind)
- [mir_eval](https://github.com/craffel/mir_eval)


## ğŸ§© Want to Contribute?

We welcome contributions! Please feel free to submit a PR or open an issue if you'd like to add new papers, tools, or correct any mistakes.

### âœ… Guidelines:
- Add relevant papers or projects related to Vision2Audio.
- Use consistent formatting.
- Include links where available.
